name: Test Zig YAML Parser

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'build.zig'
      - 'test_runner.py'
      - '.github/workflows/test-zig-parser.yml'
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'build.zig'
      - 'test_runner.py'
      - '.github/workflows/test-zig-parser.yml'

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install PyYAML
    
    - name: Setup Zig
      uses: goto-bus-stop/setup-zig@v2
      with:
        version: 0.14.1
    
    - name: Setup Node.js for TypeScript parser
      uses: actions/setup-node@v4
      with:
        node-version: '20'
    
    - name: Setup Rust for Rust parser
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        profile: minimal
    
    - name: Build parsers for testing
      run: |
        # Build Zig parser
        zig build
        
        # Build TypeScript parser
        cd yaml-ts
        npm install
        npm run build || true  # Allow to fail if already built
        cd ..
        
        # Build Rust parser
        cd yaml-rs-test
        cargo build --release
        cd ..
    
    - name: Run unit tests
      run: |
        # Run tests and capture both stdout and stderr
        zig build test --summary all 2>&1 | tee unit-test-output.txt || true
        
        # Also run parser tests directly to get detailed output if they fail
        zig test src/parser_tests.zig 2>&1 | tee parser-test-details.txt || true
    
    - name: Run YAML test suite
      run: |
        zig build test-yaml -- zig
    
    - name: Generate test report
      if: always()
      run: |
        # Capture YAML test suite results
        zig build test-yaml -- zig --verbose > test-results.txt 2>&1 || true
        
        # Add to GitHub step summary
        echo "## Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Report unit test summary from zig build test
        echo "### Unit Tests" >> $GITHUB_STEP_SUMMARY
        if [ -f unit-test-output.txt ]; then
          if grep -q "All .* tests passed" unit-test-output.txt; then
            echo "✅ All tests passed!" >> $GITHUB_STEP_SUMMARY
            UNIT_TEST_SUMMARY="✅ All unit tests passed"
          elif grep -q "Build Summary:" unit-test-output.txt; then
            # Extract summary from build output
            SUMMARY=$(grep "Build Summary:" unit-test-output.txt | head -1)
            echo "$SUMMARY" >> $GITHUB_STEP_SUMMARY
            UNIT_TEST_SUMMARY="$SUMMARY"
            
            # If tests failed, show which ones
            if grep -q "failed" unit-test-output.txt; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "#### Failed tests:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              grep "error:" unit-test-output.txt | grep "test\." | head -10 >> $GITHUB_STEP_SUMMARY || true
              echo '```' >> $GITHUB_STEP_SUMMARY
              
              # Extract failing test names for PR comment
              echo "FAILING_UNIT_TESTS<<EOF" >> $GITHUB_ENV
              # Look for test failures in both build output and parser test details
              if [ -f parser-test-details.txt ]; then
                grep "FAIL" parser-test-details.txt | sed 's/.*test\.\(.*\)\.\.\./  - \1/' | head -10 >> $GITHUB_ENV || true
              fi
              grep "'.*' failed:" unit-test-output.txt | sed "s/.*'\(.*\)' failed.*/  - \1/" | head -10 >> $GITHUB_ENV || true
              echo "EOF" >> $GITHUB_ENV
            fi
          else
            echo "Unit tests ran with issues. Check logs for details." >> $GITHUB_STEP_SUMMARY
            UNIT_TEST_SUMMARY="Unit tests: check logs for details"
          fi
        else
          echo "Unit tests were not run." >> $GITHUB_STEP_SUMMARY
          UNIT_TEST_SUMMARY="Unit tests: not run"
        fi
        
        # Add parser test details if available
        if [ -f parser-test-details.txt ]; then
          if grep -q "failed" parser-test-details.txt; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Parser Test Details:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep -E "^\d+/\d+.*\.\.\." parser-test-details.txt | grep -v "OK" | head -10 >> $GITHUB_STEP_SUMMARY || true
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
        fi
        
        echo "UNIT_TEST_SUMMARY=$UNIT_TEST_SUMMARY" >> $GITHUB_ENV
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Report YAML test suite
        echo "### YAML Test Suite" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        tail -20 test-results.txt >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        # Extract pass/fail counts and test lists for PR comment
        PASSING=$(grep "Passing:" test-results.txt | awk '{print $2, $3}')
        FAILING=$(grep "Failing:" test-results.txt | awk '{print $2}')
        echo "PASSING=$PASSING" >> $GITHUB_ENV
        echo "FAILING=$FAILING" >> $GITHUB_ENV
        
        # Extract failing test names
        echo "FAILING_TESTS<<EOF" >> $GITHUB_ENV
        grep "^  - " test-results.txt | head -20 >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV
        
        # No separate failing unit tests extraction since we use zig build test
        echo "FAILING_UNIT_TESTS<<EOF" >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV
        
        # Get baseline passing tests (from main branch)
        echo "BASELINE_PASSING=397" >> $GITHUB_ENV
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const passing = process.env.PASSING || 'N/A';
          const failing = process.env.FAILING || 'N/A';
          const failingTests = process.env.FAILING_TESTS || '';
          const unitTestSummary = process.env.UNIT_TEST_SUMMARY || 'Unit tests: not run';
          const failingUnitTests = process.env.FAILING_UNIT_TESTS || '';
          const baseline = parseInt(process.env.BASELINE_PASSING || '397');
          const currentPassing = parseInt(passing.split(' ')[0]) || 0;
          const improvement = currentPassing - baseline;
          
          // Determine overall status based on both test suites
          let overallEmoji = '✅';
          let overallStatus = 'All tests passing!';
          
          // Check unit test status
          const unitTestHasFailed = unitTestSummary.includes('failed') && !unitTestSummary.includes('0 failed');
          
          // Check YAML test suite status
          let yamlStatus = '';
          if (failing !== '0' && failing !== 'N/A') {
            if (currentPassing < baseline) {
              yamlStatus = `❌ Regression: ${baseline - currentPassing} fewer tests passing than baseline`;
              overallEmoji = '❌';
              overallStatus = 'Test regressions detected';
            } else if (currentPassing > baseline) {
              yamlStatus = `🚀 Progress: ${improvement} more tests passing than baseline`;
              overallEmoji = unitTestHasFailed ? '⚠️' : '🚀';
              overallStatus = unitTestHasFailed ? 'Making progress, but some unit tests failing' : 'Test improvements!';
            } else {
              yamlStatus = 'No change from baseline';
              overallEmoji = unitTestHasFailed ? '⚠️' : '✓';
              overallStatus = unitTestHasFailed ? 'Unit test failures' : 'No changes';
            }
          } else {
            yamlStatus = '✅ All YAML tests passing!';
          }
          
          if (unitTestHasFailed && overallEmoji === '✅') {
            overallEmoji = '⚠️';
            overallStatus = 'Unit test failures';
          }
          
          // Parse unit test summary for better formatting
          let unitTestDisplay = unitTestSummary;
          if (unitTestSummary.includes('Build Summary:')) {
            // Extract the numbers from "Build Summary: X/Y steps succeeded; A/B tests passed"
            const match = unitTestSummary.match(/(\d+)\/(\d+) tests passed(?:; (\d+) failed)?/);
            if (match) {
              const passed = match[1];
              const total = match[2];
              const failed = match[3] || '0';
              if (failed === '0') {
                unitTestDisplay = `✅ **${passed}/${total}** tests passed`;
              } else {
                unitTestDisplay = `⚠️ **${passed}/${total}** tests passed, **${failed}** failed`;
              }
            }
          }
          
          let commentBody = `## ${overallEmoji} Test Results
          
          ${overallStatus}
          
          ---
          
          ### 🧪 Unit Tests
          ${unitTestDisplay}`;
          
          if (failingUnitTests.trim()) {
            commentBody += `\n\n**Failing tests:**\n${failingUnitTests}`;
          }
          
          commentBody += `\n\n---\n\n### 📋 YAML Test Suite\n${yamlStatus}\n\n`;
          commentBody += `- **Passing**: ${passing}\n`;
          commentBody += `- **Failing**: ${failing}\n`;
          commentBody += `- **Baseline**: ${baseline} passing`;
          
          if (improvement > 0) {
            commentBody += `\n- **Improvement**: +${improvement} tests now passing! 🎉`;
          }
          
          if (failingTests.trim()) {
            commentBody += `\n\n**Failing tests:**\n${failingTests}`;
          }
          
          // Add a hidden marker to identify our bot comments
          commentBody += '\n\n<!-- yaml-test-suite-results -->';
          
          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });
          
          const botComment = comments.find(comment => 
            comment.body.includes('<!-- yaml-test-suite-results -->')
          );
          
          if (botComment) {
            // Update existing comment
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: commentBody
            });
          } else {
            // Create new comment
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });
          }
    
    - name: Check test threshold
      run: |
        # Extract the number of passing tests
        PASS_COUNT=$(grep "Passing:" test-results.txt | awk '{print $2}')
        
        # Minimum threshold (current baseline)
        MIN_PASSING=397
        
        if [ "$PASS_COUNT" -lt "$MIN_PASSING" ]; then
          echo "❌ Test regression detected!"
          echo "Expected at least $MIN_PASSING passing tests, but only $PASS_COUNT passed."
          exit 1
        else
          echo "✅ Tests passed threshold: $PASS_COUNT/$MIN_PASSING"
        fi